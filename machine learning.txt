Framing

supervised machine learning-ML systems learn how to combine input to produce useful predictions on never-before-seen data

label-A label is the thing we're predicting—the y variable in simple linear regression

linear regression-
Linear Regression is a supervised machine learning algorithm where the predicted output is continuous and has a constant slope. It’s used to predict values within a continuous range, (e.g. sales, price) rather than trying to classify them into categories (e.g. cat, dog). There are two main types:
Simple regression
Multivariable regression

Simple regression-
Simple linear regression uses traditional slope-intercept form, where m and b are the variables our algorithm will try to learn to produce the most accurate predictions. x represents our input data and y represents our prediction.
y=mx+b

Multivariable regression-
A more complex, multi-variable linear equation might look like this, where w represents the coefficients, or weights, our model will try to learn.

f(x,y,z)=w1x+w2y+w3z
The variables x,y,z represent the attributes, or distinct pieces of information, we have about each observation. For sales predictions, these attributes might include a company’s advertising spend on radio, TV, and newspapers.

Sales=w1Radio+w2TV+w3News

A feature is an input variable—the x variable in simple linear regression. A simple machine learning project might use a single feature, while a more sophisticated machine learning project could use millions of features, specified as:

x1,x2....xn

model
A machine learning model is a file that has been trained to recognize certain types of patterns. You train a model over a set of data, providing it an algorithm that it can use to reason over and learn from those data.


Training and loss
Training a model simply means learning (determining) good values for all the weights and the bias from labeled examples. In supervised learning, a machine learning algorithm builds a model by examining many examples and attempting to find a model that minimizes loss; this process is called empirical risk minimization.

Loss is the penalty for a bad prediction. That is, loss is a number indicating how bad the model's prediction was on a single example. If the model's prediction is perfect, the loss is zero; otherwise, the loss is greater. The goal of training a model is to find a set of weights and biases that have low loss, on average, across all examples.

A Machine Learning model is trained by starting with an initial guess for the weights and bias and iteratively adjusting those guesses until learning the weights and bias with the lowest possible loss.

Squared loss: a popular loss function-
the square of the difference between the label and the prediction
= (observation - prediction(x))2
= (y - y')2

reducing loss aproaches-
An Iterative Approach-An iteration is a term used in machine learning and indicates the number of times the algorithm's parameters are updated
